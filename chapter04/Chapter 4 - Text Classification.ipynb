{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "<h1>Chapter 4 - Text Classification 文本分类</h1>\n",
        "<i>Classifying text with both representative and generative models</i>\n",
        "<i>使用表示模型和生成模型进行文本分类</i>\n",
        "\n",
        "<a href=\"https://github.com/Zijun-Yin/hands-on-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Zijun-Yin/hands-on-llm/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb)\n",
        "\n",
        "---\n",
        "这是《图解大模型》的第4章代码。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN-kg56ba1pN"
      },
      "source": [
        "### [可选] - 下载所需的 Python 包 <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "如果你在Google Colab（或任何其他云供应商）上查看这个笔记本，你需要**取消注释并运行**以下代码块来为这一章安装依赖项：\n",
        "\n",
        "---\n",
        "\n",
        "在Google Colab中，转到**运行时 > 更改运行时类型 > 硬件加速器 > GPU > GPU类型 > T4**。\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vr9j1WX-a1pO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers sentence-transformers openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "kResU5TOOFRJ",
        "outputId": "afc87ec7-d171-4a3f-d0c7-e7784d450901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeVnXxQWy7-"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "5phRS_z2U_3T",
        "outputId": "349fef3f-b159-471e-b7a5-bc1f94871064"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid pattern: '**' can only be an entire path component",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2052510088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cornell-movie-review-data/rotten_tomatoes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m         \u001b[0mdataset_path\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m             \u001b[0mPath\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\"dataset/train\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0mURI\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\"s3://my-bucket/dataset/train\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2113\u001b[0m             \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdict\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0mloaded\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_files\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m     config_name = builder_kwargs.pop(\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0;34m\"config_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_config_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         ).get_module()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0;31m#   -> use the packaged module (json, csv, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m     \u001b[0;31m# - if os.path.join(path, name) is a local python file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m     \u001b[0;31m#   -> use the module from the python file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;31m# - if path is a local directory (but no python file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         )\n\u001b[1;32m   1033\u001b[0m         \u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PACKAGED_DATASETS_MODULES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m             builder_configs, default_config_name = create_builder_configs_from_metadata_configs(\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns\u001b[0;34m(base_path, download_config)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00003.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0000\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00003.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0000\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00003.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00001.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m00003.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPurePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ]\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_directories_in_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_directories_in_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mUsers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmariosasko\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mDesktop\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnix\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mURLs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0mto\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mabsolute\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expand_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_file_system.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"expand_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def find(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/utils.py\u001b[0m in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"**\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0;34m\"Invalid pattern: '**' can only be an entire path component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our data\n",
        "# https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes\n",
        "data = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJJmaJzHDLZv"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya5dfmVoR1R"
      },
      "source": [
        "# **Text Classification with Representation Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co68g-Eloknf"
      },
      "source": [
        "## **Using a Task-specific Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph-3T3XJopdN"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Path to our HF model\n",
        "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# Load model into pipeline\n",
        "pipe = pipeline(\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    return_all_scores=True,\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2gbnL5Q69Y5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "# Run inference\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
        "    negative_score = output[0][\"score\"]\n",
        "    positive_score = output[2][\"score\"]\n",
        "    assignment = np.argmax([negative_score, positive_score])\n",
        "    y_pred.append(assignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fum3MTSyymlW"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3WT4jzoNZE"
      },
      "source": [
        "## **Classification Tasks that Leverage Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yuSP3heMzT"
      },
      "source": [
        "### Supervised Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGV9VS4bhq7f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert text to embeddings\n",
        "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
        "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5L5CLcOxIeA"
      },
      "outputs": [],
      "source": [
        "train_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A7oTxoph6bn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a Logistic Regression on our train embeddings\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(train_embeddings, data[\"train\"][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFvO9KhMokF7"
      },
      "outputs": [],
      "source": [
        "# Predict previously unseen instances\n",
        "y_pred = clf.predict(test_embeddings)\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGIHxXpJgrC"
      },
      "source": [
        "**Tip!**  \n",
        "\n",
        "What would happen if we would not use a classifier at all? Instead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f_DnG1uJ7Sk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Average the embeddings of all documents in each target label\n",
        "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
        "averaged_target_embeddings = df.groupby(768).mean().values\n",
        "\n",
        "# Find the best matching embeddings between evaluation documents and target embeddings\n",
        "sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWdzjMIjzx0"
      },
      "source": [
        "### Zero-shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSj6CdAetsNp"
      },
      "outputs": [],
      "source": [
        "# Create embeddings for our labels\n",
        "label_embeddings = model.encode([\"A negative review\",  \"A positive review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEIN7XnbtsQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Find the best matching label for each document\n",
        "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LyeuEUxIbW"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox27Rg71zclg"
      },
      "source": [
        "**Tip!**  \n",
        "\n",
        "What would happen if you were to use different descriptions? Use **\"A very negative movie review\"** and **\"A very positive movie review\"** to see what happens!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CC9iEGcuUit"
      },
      "source": [
        "## **Classification with Generative Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFPPzUHoEESB"
      },
      "source": [
        "### Encoder-decoder Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVbTUMktEfJ3"
      },
      "outputs": [],
      "source": [
        "# Load our model\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-small\",\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5nWQORcFlNn"
      },
      "outputs": [],
      "source": [
        "# Prepare our data\n",
        "prompt = \"Is the following sentence positive or negative? \"\n",
        "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nas574KFFSvR"
      },
      "outputs": [],
      "source": [
        "# Run inference\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
        "    text = output[0][\"generated_text\"]\n",
        "    y_pred.append(0 if text == \"negative\" else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wk2i856GnCv"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4V9iq_EELWx"
      },
      "source": [
        "### ChatGPT for Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tES6HFOwNjF6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Create client\n",
        "client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGiovm3wyCOz"
      },
      "outputs": [],
      "source": [
        "def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-0125\"):\n",
        "    \"\"\"Generate an output based on a prompt and an input document.\"\"\"\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\n",
        "            }\n",
        "    ]\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model=model,\n",
        "      temperature=0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL_kMwQEvMcd"
      },
      "outputs": [],
      "source": [
        "# Define a prompt template as a base\n",
        "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
        "\n",
        "[DOCUMENT]\n",
        "\n",
        "If it is positive return 1 and if it is negative return 0. Do not give any other answers.\n",
        "\"\"\"\n",
        "\n",
        "# Predict the target using GPT\n",
        "document = \"unpretentious , charming , quirky , original\"\n",
        "chatgpt_generation(prompt, document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-8XYpY3jp6"
      },
      "source": [
        "The next step would be to run one of OpenAI's model against the entire evaluation dataset. However, only run this when you have sufficient tokens as this will call the API for the entire test dataset (1066 records)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEyGElIv25Aq"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you want to save your (free) credits\n",
        "predictions = [chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07O8wtZ05x1"
      },
      "outputs": [],
      "source": [
        "# Extract predictions\n",
        "y_pred = [int(pred) for pred in predictions]\n",
        "\n",
        "# Evaluate performance\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}